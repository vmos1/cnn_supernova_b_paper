{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "Read stored model and test on custom dataset\\\n",
    "Sep 1, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import subprocess as sp\n",
    "import pickle\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "## M-L modules\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks  # or tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_get_data(prefix,data_dir):\n",
    "    '''\n",
    "    Function to get data from .npy files into images, labels and IDs.\n",
    "    '''\n",
    "    \n",
    "    img_name=data_dir+prefix+'_x.npy'\n",
    "    print(\"Input file name\",img_name)\n",
    "    try:\n",
    "        images=np.load(img_name)\n",
    "        labels=np.load(data_dir+prefix+'_y.npy')\n",
    "        ids=np.load(data_dir+prefix+'_idx.npy')\n",
    "    except Exception as e:\n",
    "        print(\"Encountered exception\",e)\n",
    "        raise SystemExit\n",
    "\n",
    "    keys=['images','labels','ids']   \n",
    "    print(images.shape,labels.shape,ids.shape)\n",
    "    values_dict=dict(zip(keys,[images,labels,ids]))\n",
    "    \n",
    "    return values_dict\n",
    "\n",
    "###############################\n",
    "### Classes used ###\n",
    "###############################\n",
    "class dataset:\n",
    "    '''\n",
    "    Class to store datasets. Example objects: train_data,val_data,test_data\n",
    "    3 arguments : \n",
    "    - Name of dataset\n",
    "    - dictionary containing x,y,ids\n",
    "    - Index array to select specific rows\n",
    "    '''\n",
    "    \n",
    "    ## Eg: dataset('train',data_dir)\n",
    "    def __init__(self,name,data_dict,idx_arr):\n",
    "        self.name=name\n",
    "\n",
    "        self.x,self.y,self.id=data_dict['images'][idx_arr],data_dict['labels'][idx_arr],data_dict['ids'][idx_arr]\n",
    "\n",
    "\n",
    "class cnn_model:\n",
    "    '''\n",
    "    Class to store features of cnn model such as : model_name, wts_filename, history_filename,\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,model_name,model_save_dir,results_dir):\n",
    "        \n",
    "        ### Initialization ###\n",
    "        self.name=model_name\n",
    "        #Declare names of files for storing model, model weights, history\n",
    "        self.fname_model=model_save_dir+'model_{0}.h5'.format(model_name)\n",
    "        self.fname_model_wts=model_save_dir+'model_wts_{0}.h5'.format(model_name)\n",
    "        self.fname_history=model_save_dir+'history_{0}.pickle'.format(model_name)\n",
    "        self.fname_ypred=results_dir+'ypred_{0}.test'.format(model_name)\n",
    "        self.fname_id_test=results_dir+'id_test_{0}.test'.format(model_name)\n",
    "        self.fname_ytest=results_dir+'ytest_{0}.test'.format(model_name)\n",
    "        \n",
    "    def f_build_model(self,model):\n",
    "        '''Store model in the class member. Reads in a keras model   '''\n",
    "        self.cnn_model=model\n",
    "    \n",
    "    def f_train_model(self,train_data,val_data,num_epochs=5,batch_size=64):\n",
    "#         model,inpx,inpy,model_weights):\n",
    "        '''\n",
    "        Train model. Returns just history.history\n",
    "        '''\n",
    "    \n",
    "        def f_learnrate_sch(epoch,lr):\n",
    "            ''' Module to schedule the learn rate'''\n",
    "            step=10 ### learn rate is constant up to here\n",
    "            #if epoch>step: lr=lr*np.exp(-0.2*(epoch-10)) # Exponential decay after 10\n",
    "            if (epoch>=step and epoch%step==0): lr=lr/2.0\n",
    "             \n",
    "            return lr \n",
    "    \n",
    "        callbacks_lst=[]\n",
    "        callbacks_lst.append(callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=1))\n",
    "        callbacks_lst.append(callbacks.ModelCheckpoint(self.fname_model_wts, save_best_only=True, monitor='val_loss', mode='min'))\n",
    "        callbacks_lst.append(callbacks.LearningRateScheduler(f_learnrate_sch,verbose=1))\n",
    "         \n",
    "        model=self.cnn_model\n",
    "        history=model.fit(x=train_data.x, y=train_data.y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks_lst,\n",
    "                        #validation_split=cv_fraction,\n",
    "                        validation_data=(val_data.x,val_data.y),\n",
    "                        shuffle=True)\n",
    "        \n",
    "        print(\"Number of parameters\",model.count_params())\n",
    "\n",
    "        self.history=history.history       \n",
    "\n",
    "    \n",
    "    def f_save_model_history(self):\n",
    "        ''' Save model and history'''\n",
    "        \n",
    "        self.cnn_model.save(self.fname_model)\n",
    "        with open(self.fname_history, 'wb') as f:\n",
    "            pickle.dump(self.history, f)\n",
    "    \n",
    "    def f_load_model_history(self):\n",
    "        ''' For pre-trained model, load model and history'''\n",
    "        \n",
    "        ## Check if files exist\n",
    "        assert os.path.exists(self.fname_model),\"Model not saved: %s\"%(self.fname_model)\n",
    "        assert os.path.exists(self.fname_history),\"History not saved: %s\"%(self.fname_history)\n",
    "        \n",
    "        ## Load data from files\n",
    "        self.cnn_model=load_model(self.fname_model)\n",
    "        with open(self.fname_history,'rb') as f:  history= pickle.load(f)        \n",
    "    \n",
    "    def f_test_model(self,test_data):\n",
    "        '''\n",
    "        Test model and return array with predictions\n",
    "        '''\n",
    "        model=self.cnn_model\n",
    "#         model.evaluate(test_data.x,test_data.y,sample_weights=wts,verbose=1)\n",
    "        print(test_data.x.shape)\n",
    "        y_pred=model.predict(test_data.x,verbose=1)\n",
    "        ### Ensure prediction has the same size as labelled data.\n",
    "        \n",
    "        assert(test_data.y.shape[0]==y_pred.shape[0]),\"Data %s and prediction arrays %s are not of the same size\"%(test_data.y.shape,y_pred.shape)\n",
    "        \n",
    "        ##Condition for the case when the prediction is a 2column array \n",
    "        ## This complicated condition is needed since the array has shape (n,1) when created, but share (n,) when read from file.\n",
    "        if (len(y_pred.shape)==2 and y_pred.shape[1]==2) : y_pred=y_pred[:,1]\n",
    "        \n",
    "        ### Store predictions to the class object\n",
    "        self.y_pred=y_pred\n",
    "    \n",
    "    def f_save_predictions(self,test_data):\n",
    "        ''' Save predictions for test data and the actual test data labels\n",
    "            Also save IDs of train and validation data'''\n",
    "        \n",
    "        ## Save the predictions on test data for the labels, for roc curve\n",
    "        np.savetxt(self.fname_ypred,self.y_pred)\n",
    "        \n",
    "        ## Save the test data labels and IDs for roc curve \n",
    "        ### This is just the test data, but it is useful to save it, to make the analysis part simpler\n",
    "        np.savetxt(self.fname_ytest,test_data.y)\n",
    "        ### Save IDs of test data\n",
    "        np.savetxt(self.fname_id_test,test_data.id)\n",
    "\n",
    "class trained_model:\n",
    "    '''\n",
    "    Class to extract data of trained model\n",
    "    variables: model,history, y_pred (predictions of labels), fpr, tpr, threshold, auc\n",
    "    functions: f_read_stored_model, f_compute_preds\n",
    "    Example objects :  (models numbers) '1', '2', etc.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,model_name,model_save_dir):\n",
    "        \n",
    "        self.tpr,self.fpr,self.threshold,self.auc=[],[],[],None\n",
    "        self.precision,self.recall,self.threshold2,self.fscore,self.auc2=[],[],[],[],None\n",
    "        self.f_read_stored_model(model_name,model_save_dir)\n",
    "        \n",
    "    def f_read_stored_model(self,model_name,model_save_dir):\n",
    "        '''\n",
    "        Read model, history and predictions\n",
    "        '''\n",
    "        \n",
    "        fname_model='model_{0}.h5'.format(model_name)\n",
    "        fname_history='history_{0}.pickle'.format(model_name)\n",
    "\n",
    "        # Load model and history\n",
    "        self.model=load_model(model_save_dir+fname_model)\n",
    "        \n",
    "        with open(model_save_dir+fname_history,'rb') as f:\n",
    "            self.history= pickle.load(f)\n",
    "        \n",
    "        # Load predictions\n",
    "        fname_ypred=model_save_dir+'ypred_{0}.test'.format(model_name)\n",
    "        self.y_pred=np.loadtxt(fname_ypred)\n",
    "\n",
    "        # Load true labels\n",
    "        fname_ytest=model_save_dir+'ytest_{0}.test'.format(model_name)\n",
    "        self.y_test=np.loadtxt(fname_ytest)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def f_compute_preds(self):\n",
    "        '''\n",
    "        Module to use model and compute \n",
    "        '''\n",
    "        \n",
    "        y_pred=self.y_pred\n",
    "        test_y=self.y_test\n",
    "#         print(test_x.shape,test_y.shape,y_pred.shape)\n",
    "\n",
    "        ## roc curve\n",
    "        self.fpr,self.tpr,self.threshold=roc_curve(test_y,y_pred)\n",
    "        # AUC \n",
    "        self.auc= auc(self.fpr, self.tpr)\n",
    "        \n",
    "        # calculate precision-recall curve\n",
    "        self.precision, self.recall, self.thresholds2 = precision_recall_curve(test_y, y_pred)\n",
    "#         self.precision, self.recall, self.fscore, support = precision_recall_fscore_support(test_y, y_pred, sample_weight=test_wts)\n",
    "        \n",
    "        # AUC2\n",
    "        self.auc2= auc(self.recall, self.precision)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir='/global/cfs/cdirs/dasrepo/vpa/supernova_cnn/data/results_data/results/final_summary_data_folder/'\n",
    "model_save_dir=main_dir+'saved_models/'\n",
    "data_dir=main_dir+'sample_test_data/'\n",
    "results_dir=main_dir+'results_inference/'\n",
    "# data_dir='/global/cfs/cdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/input_npy_files/'\n",
    "prefix='input_test'\n",
    "# prefix='full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file name /global/cfs/cdirs/dasrepo/vpa/supernova_cnn/data/results_data/results/final_summary_data_folder/sample_test_data/input_test_x.npy\n",
      "(5000, 51, 51, 3) (5000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "data_dict=f_get_data(prefix,data_dir)\n",
    "size=len(data_dict['labels'])\n",
    "data=dataset('test',data_dict,np.arange(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create dataset using indices\n",
    "\n",
    "# #### Read data from files \n",
    "# data_dict=f_get_data(prefix,data_dir)\n",
    "\n",
    "# t1=time.time()\n",
    "\n",
    "# size_data=data_dict['labels'].shape[0]\n",
    "# print(\"Size of entire dataset is : \",size_data)\n",
    "# #### Define the indices for training, validation and test data\n",
    "# train_size,val_size,test_size=int(0.5*size_data),int(0.05*size_data),int(0.05*size_data)\n",
    "# # train_size,val_size,test_size=int(0.0*size_data),int(0.0*size_data),int(1.0*size_data)\n",
    "\n",
    "\n",
    "# ### Get random indices for test,train,val\n",
    "# np.random.seed(225) # Set random seed\n",
    "# test_idx=np.random.choice(np.arange(size_data),test_size,replace=False)\n",
    "# # get remaining indices without test indices\n",
    "# rem_idx1=np.array(list(set(np.arange(size_data))-set(test_idx)))\n",
    "# val_idx=np.random.choice(rem_idx1,val_size,replace=False)\n",
    "# rem_idx2=np.array(list(set(rem_idx1)-set(val_idx)))\n",
    "# train_idx=np.random.choice(rem_idx2,train_size,replace=False)\n",
    "\n",
    "# print(\"Shapes of indices\",train_idx.shape, val_idx.shape, test_idx.shape)\n",
    "\n",
    "# #### Storing arrays into train,validation, test objects and deleting the full data dictionary\n",
    "# train_data=dataset('training',data_dict,train_idx)\n",
    "# val_data=dataset('validation',data_dict,val_idx)\n",
    "# test_data=dataset('test',data_dict,test_idx)\n",
    "# del data_dict\n",
    "# # print(\"\\nData shapes: Train {0}, Validation {1}, Test {2}\\n\".format(train_data.x.shape,val_data.x.shape,test_data.x.shape))\n",
    "\n",
    "# t2=time.time()\n",
    "# print(\"Time taken to read and process input files\",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc='/global/cfs/cdirs/dasrepo/vpa/supernova_cnn/data/results_data/results/final_summary_data_folder/sample_test_data/'\n",
    "# fname=loc+'input_test_idx.npy'\n",
    "# np.save(fname,test_data.id[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(fname).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(5000, 51, 51, 3)\n",
      "157/157 [==============================] - 3s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "for model_name in [str(i) for i in [1]]:\n",
    "    print(model_name)\n",
    "    ### Define Object for cnn_model\n",
    "    Model=cnn_model(model_name,model_save_dir,results_dir)\n",
    "\n",
    "    ### Read stored model and history\n",
    "    Model.f_load_model_history()\n",
    "\n",
    "    #################################\n",
    "    ### Test model ###\n",
    "    Model.f_test_model(data)\n",
    "\n",
    "    ## Save prediction array and labels array\n",
    "    Model.f_save_predictions(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3",
   "language": "python",
   "name": "v-jpt-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
